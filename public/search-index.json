[
  {
    "title": "Consideration of a Generative Method Using a Gaussian Mixture Model",
    "summary": "We constructed a basic generative model using a Gaussian Mixture Model (GMM).",
    "tags": [
      "python",
      "Machine Learning",
      "Data Science",
      "Mathematical Optimization"
    ],
    "slug": "Generative_AI/GMM_gen_ai",
    "content": "\n# Gaussian Mixture Model (GMM)\n\n- A clustering method.\n- Can also be used as a generative model.\n- Represents a given dataset as a combination of multiple Gaussian distributions.\n- Provides a probability density function, which explains its use as a generative model.\n- Can automatically determine the number of clusters.\n- Reveals the prior distribution of explanatory variable X (latent variable).\n\n## Gaussian Distribution (can be visualized as a graph in a two-dimensional space of x and y)\n\n$$\nN(x|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\n$$\n\n## Multivariate Normal Distribution (Can be viewed as a graph in three or more dimensions such as x, y, z...)\n\n$$\nN(\\mathbf x|\\mathbf\\mu,\\Sigma)=\\frac{1}{(2\\pi)^\\frac{m}{2}}\\frac{1}{|\\Sigma|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}(\\mathbf x-\\mathbf\\mu)^T\\Sigma^{-1}(\\mathbf x-\\mathbf\\mu)\\right\\}\\quad\\left(\\mathbf x:\nRandom~variable~vector,\\mathbf\\mu:Average~vector,\\Sigma:Covariance~matrix\\right)\n$$\n\n## Mixture of Gaussian Distribution (Multiple peaks are formed)\n\n$$\np(\\mathbf x)=\\sum_{k=1}^n\\pi_kN(\\mathbf x|\\mu_k,\\Sigma_k)\\quad\\left(n:n ~Gaussian~distributions,k:k-th~Gaussian~distribution,\\pi_k:Mixture~coefficient\\left(Weight~of~each~Gaussian~distribution,\\sum_{k=1}^n\\pi_k=1\\right)\\right)\n$$\n\nReference: https://datachemeng.com/wp-content/uploads/gaussianmixturemodel.pdf\n\n# Prior distribution of Mixture Gaussian Distribution\n\nThe prior distribution here is the distribution of \"which cluster a variable belongs to after receiving it.\" If we define the latent variable as $\\mathbf z$, then $\\mathbf z$ is\n\n- A vector (i.e., a matrix) with a one-hot vector that is 1 in one cluster and 0 in the others.\n- If there is no information about the sample, the probability that $\\mathbf z_k=1$ is set to follow the mixture coefficient.\n  $$\n  p(\\mathbf z_k=1)=\\pi_k\n  $$\n\nIf this is not introduced, the area will not be 1 in the mixture Gaussian distribution (it is obvious that the sum of probabilities will exceed 1 if simply added).\n\n## Finding the prior distribution\n\nUsing Bayes' theorem, the probability that a sample $\\mathbf x$ will be $z_k=1$ given is\n\n$$\np(z_k=1|\\mathbf x)=\\frac{p(z_k=1)p(\\mathbf x|z_k=1)}{\\sum_{i=1}^np(z_k=1)p(\\mathbf x|z_i=1)}=\\frac{\\pi_kp(\\mathbf x|z_k=1)}{\\sum_{i=1}^n\\pi_ip(\\mathbf x|z_i=1)}=\\frac{\\pi_kN(\\mathbf x|\\mu_k,\\Sigma_k)}{\\sum_{i=1}^n\\pi_iN(\\mathbf x|\\mu_i,\\Sigma_i)}\n$$\n\nFrom this,\n\n$$\nk^\\star=\\arg\\max_kp(z_k=1|\\mathbf x)\n$$\n\nBy doing so, the cluster at a certain point $\\mathbf x$ can be estimated.\n\n# Creating a Mixture Gaussian Distribution\n\n## Definition of probability density function\n\n```py\n# Multivariate normal distribution\ndef gaussian_densty(x, mu, sigma): # (1, n), (1, n), (n, n)\n  diff = x - mu\n  sigma_inv = np.linalg.inv(sigma)\n  sigma_det = np.linalg.det(sigma)\n  z = np.exp(-np.dot(diff.T, np.dot(sigma_inv, diff)) / 2)\n\n  return z / np.sqrt(np.power(2*np.pi, len(mu)) * sigma_det) # (1, n)\n\n# Mixture Gaussian distribution\ndef mixture_gaussian_densty(x, mu_list, sigma_list, pi_list): # (1, n), (k, n), (k, n, n), (1, k)\n  z = 0\n  for i in range(len(pi_list)):\n    z += pi_list[i] * gaussian_densty(x, mu_list[i], sigma_list[i])\n  return z # (1, n)\n```\n\n> Note that only the probability density function has been created, so the corresponding code is required to sample it.\n\n## Using NumPy's Official Multivariate Normal Distribution\n\n```py\n# Generate samples from a multivariate normal distribution\ndef sample_multivariate_gaussian(mu, sigma, num_samples=1):\n    return np.random.multivariate_normal(mu, sigma, num_samples)\n\n# Generate samples from a mixture Gaussian distribution\ndef sample_mixture_gaussian(mu_list, sigma_list, pi_list, num_samples=100):\n    samples = []\n    num_clusters = len(pi_list)\n\n    # Determine the number of samples in each cluster\n    cluster_sizes = np.random.multinomial(num_samples, pi_list) # Randomly determine the number of samples in each cluster using the mixture coefficients\n\n    for i in range(num_clusters):\n        # Generate samples for each cluster\n        cluster_samples = sample_multivariate_gaussian(mu_list[i], sigma_list[i], cluster_sizes[i])\n        samples.append(cluster_samples)\n\n    # Combine the samples and return them\n    return np.vstack(samples)\n```\n\n## Preparing the data\n\n```py\n# Prepare sample data\n# Create a Gaussian mixture distribution with random variable X = [x0, x1], Mu = [mu0, mu1], and latent variable Z = [z0, z1, z2].\n\n# Cluster 0\nmu0 = np.array([0, -0.5])\nsigma0 = np.array([[1.0, 0], [0, 1.0]])\n# Cluster 1\nmu1 = np.array([2.5, 2])\nsigma1 = np.array([[0.5, 0.3], [0.3, 0.7]])\n\n# Cluster 2\nmu2 = np.array([-2, 1.5])\nsigma2 = np.array([[1.2, 0.2], [0.2, 0.4]])\n\n# Combine data\nmu_list = [mu0, mu1, mu2]\nsigma_list = [sigma0, sigma1, sigma2]\npi_list = [0.45, 0.25, 0.3]\n\n# Number of sample data points\nNUM_DATA = 500\n\n# Generate samples from the Gaussian mixture distribution\nsamples = sample_mixture_gaussian(mu_list, sigma_list, pi_list, num_samples=NUM_DATA)\n\nsamples[:10]\n```\n\n## Plotting the Gaussian Mixture Distribution\n\n```py\ndef plot_mixture_gaussian(mu_list, sigma_list, pi_list, samples=None, figsize=(8,6)):\n    x_range = np.linspace(-5, 5, 100)\n    y_range = np.linspace(-5, 5, 100)\n    X, Y = np.meshgrid(x_range, y_range)\n    Z = np.zeros((len(x_range), len(y_range)))\n\n    # Calculate the probability density for each coordinate\n    for i in range(len(x_range)):\n        for j in range(len(y_range)):\n            x = np.array([x_range[i], y_range[j]])\n            Z[i, j] = mixture_gaussian_densty(x, mu_list, sigma_list, pi_list)\n\n    plt.figure(figsize=figsize)\n    if samples is not None:\n        plt.scatter(samples[:, 1], samples[:, 0], color='red', s=10, label='Samples')\n    plt.contour(X, Y, Z, cmap='viridis')\n    plt.colorbar(label='Density')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.title('Mixture Gaussian Distribution')\n    plt.show()\n\nplot_mixture_gaussian(mu_list, sigma_list, pi_list, samples)\n```\n\n# Estimating Clusters from Samples\n\n`To be considered later`\n\n# Estimating Parameters of the Mixture Gaussian Distribution from Samples\n\n## Maximum Likelihood Estimation\n\n\"Likelihood maximization\" is one of the methods to estimate the distribution from the sample data.\n\n$$\nLikelihood~function:{\\cal L}(\\theta;\\mathbf x)=\\prod_{i=1}^nP(x_i;\\theta),\\quad Maximum~likelihood~estimator:\\theta^\\star=\\arg\\max_\\theta{\\cal L}(\\theta;\\mathbf x)\\quad(n:Number~of~samples)\n$$\n\nReference: https://cochineal19.hatenablog.com/entry/2021/11/08/003751\n\n## Define the log likelihood function\n\nIt's hard to calculate the likelihood function directly because it's a product of many probabilities. Therefore, it is common to use the log likelihood function.\n\nThere is no effect on the maximum value of the function because the logarithm is a monotonically increasing function.\n\n$$\n{\\cal L}(\\mu,\\Sigma,\\pi;\\mathbf x)=\\prod_{i=1}^np(\\mathbf x;\\mu,\\Sigma,\\pi)=\\prod_{i=1}^n\\sum_{k=1}^n\\pi_kN(\\mathbf x|\\mu_k,\\Sigma_k)\\\\\\to\\log{\\cal L}(\\mu,\\Sigma,\\pi;\\mathbf x)=\\log\\prod_{i=1}^n\\sum_{k=1}^n\\pi_kN(\\mathbf x|\\mu_k,\\Sigma_k)=\\sum_{i=1}^n\\log\\sum_{k=1}^n\\pi_kN(\\mathbf x|\\mu_k,\\Sigma_k)\n$$\n\n```py\ndef log_likelihood(mu_list, sigma_list, pi_list, sample):\n  log_likelihood = 0\n  for i in range(len(sample)):\n    log_likelihood += np.log(mixture_gaussian_densty(sample[i], mu_list, sigma_list, pi_list))\n\n  return log_likelihood\n```\n\n## Defining the Burden Rate\n\nSince we need to find the prior distribution \\( p(z|x) \\) of \\( p(x|z) \\), the prior probability of each cluster for a given data point \\( \\mathbf{x_i} \\) is expressed as follows:\n\n$$\np_{\\mu,\\Sigma,\\pi}(z_{ik}=1|\\mathbf{x_i}) = \\frac{\\pi_k{\\cal N}(\\mathbf{x_i};\\mu_k,\\Sigma_k)}{\\sum_{j=1}^K \\pi_j {\\cal N}(\\mathbf{x_i};\\mu_j,\\Sigma_j)} \\equiv \\gamma(z_{ik})\n$$\n\nIntuitively, we are calculating the probability that the distribution of each cluster fits the given data point.\n\n```py\ndef responsibility(data, mu_list, sigma_list, pi_list):\n    gamma = np.zeros((len(data), len(pi_list)))\n    for i in range(len(data)):\n        for j in range(len(pi_list)):\n            gamma[i, j] = pi_list[j] * gaussian_density(data[i], mu_list[j], sigma_list[j])\n        gamma[i] /= np.sum(gamma[i])\n    return gamma\n```\n\n### Classifying Using the Burden Rate\n\nTo perform classification, we need to find:\n\n$$\nz^\\star = \\argmax_{z_{ik}} \\gamma(z_{ik})\n$$\n\nThis gives us the cluster with the highest responsibility for each data point.\n\n```python\ndf = pd.DataFrame(samples[:, [1, 0]], columns=['x', 'y'])\ngamma = responsibility(samples, mu_list, sigma_list, pi_list)\ndf['gamma0'] = gamma[:, 0]\ndf['gamma1'] = gamma[:, 1]\ndf['gamma2'] = gamma[:, 2]\ndf['z_star'] = df[['gamma0', 'gamma1', 'gamma2']].idxmax(axis=1)\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x_range, y_range)\nZ = np.zeros((len(x_range), len(y_range)))\n\n# Calculate probability density for each coordinate\nfor i in range(len(x_range)):\n    for j in range(len(y_range)):\n        x = np.array([x_range[i], y_range[j]])\n        Z[i, j] = mixture_gaussian_density(x, mu_list, sigma_list, pi_list)\n\nsns.scatterplot(x='x', y='y', hue='z_star', data=df, palette='viridis')\nplt.contour(X, Y, Z, cmap='viridis')\nplt.colorbar(label='Density')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Mixture Gaussian Distribution')\nplt.show()\n```\n\n## Learning\n\nTo maximize the log-likelihood, we need to solve:\n\n$$\n\\argmax_{\\mu,\\Sigma,\\pi} \\log {\\cal L}(\\mu, \\Sigma, \\pi; \\mathbf{x}) = \\argmax_{\\mu,\\Sigma,\\pi} \\sum_{i=1}^n \\log \\sum_{k=1}^K \\pi_k N(\\mathbf{x} | \\mu_k, \\Sigma_k)\n$$\n\nIn this case, the log-sum part makes it difficult to solve analytically â†’ We optimize it using the Expectation-Maximization (EM) algorithm.\n\n### Initialize the parameters\n\nSet randomly\n\n### E-Step\n\nCalculate the responsibility for each data point at the current step:\n\n$$\np_{\\mu,\\Sigma,\\pi}(z_{ik}=1|\\mathbf x_i)=\\frac{\\pi_k{\\cal N}(\\mathbf x_i;\\mu_k,\\Sigma_k)}{\\sum_{j=1}^K\\pi_j{\\cal N}(\\mathbf x_i;\\mu_j,\\Sigma_j)}\\equiv\\gamma(z_{ik})\n$$\n\n### M-Step\n\nUpdate the parameters to maximize the likelihood using the responsibilities:\n\n$$\n\\begin{aligned}\n&\\frac{\\partial{\\cal L}}{\\partial\\mu_k}=0\\to\\mu_k=\\frac{1}{N_k}\\sum_{i=1}^n\\gamma(z_{ik})\\mathbf x_i\\\\\n&\\frac{\\partial{\\cal L}}{\\partial\\Sigma_k}=0\\to\\Sigma_k=\\frac{1}{N_k}\\sum_{i=1}^n\\gamma(z_{ik})(\\mathbf x_i-\\mu_k)(\\mathbf x_i-\\mu_k)^T\n\\end{aligned}\n\\quad\\left(N_k=\\sum_{i=1}^n\\gamma(z_{ik})\\right)\n$$\n\nFor $\\pi_k$, since $\\sum_{k=1}^K\\pi_k=1$, we use the Lagrange multiplier method to maximize the likelihood:\n\n$$\n\\frac{\\partial G}{\\partial\\pi_k}=0\\to\\pi_k=\\frac{N_k}{\\sum_{k=1}^KN_k}\\quad\\left(G={\\cal L}+\\lambda\\left(\\sum_{k=1}^K\\pi_k-1\\right)\\right)\n$$\n\n### Convergence Condition\n\nIf the change in the likelihood meets a predefined threshold $\\epsilon$:\n\n$$\n{\\cal L}_{new}-{\\cal L}_{old}<\\epsilon\n$$\n\nthen stop the iterations; otherwise, repeat the EM steps.\n\n```python\n# Number of clusters (You can use methods to find the optimal number)\nK = 3\n\n# Initialize random mean vectors (2D)\nmu_list = [np.random.randn(2) for _ in range(K)]\n\n# Initialize random covariance matrices (2x2)\nsigma_list = []\nfor _ in range(K):\n    A = np.random.randn(2, 2)\n    sigma = np.dot(A, A.T)  # Create symmetric and positive definite matrices\n    sigma_list.append(sigma)\n\n# Initialize random mixing coefficients and normalize\npi_list = np.random.rand(K)\npi_list = pi_list / np.sum(pi_list)\n\n# Display the results\nprint(\"mu_list:\", mu_list)\nprint(\"sigma_list:\", sigma_list)\nprint(\"pi_list:\", pi_list)\n\nn_iter = 0\n\nlikely = log_likelihood(mu_list, sigma_list, pi_list, samples) / NUM_DATA\nprint('Iteration: {0}, log_likelihood: {1}'.format(n_iter, likely))\nplot_mixture_gaussian(mu_list, sigma_list, pi_list, samples, figsize=(4,3))\n\nth = 0.0001\n\nwhile True:\n  n_iter += 1\n\n  # E-Step\n  gamma = responsibility(samples, mu_list, sigma_list, pi_list)\n  n_k = np.sum(gamma, axis=0)\n\n  # M-Step\n\n  # Update pi\n  pi_list_next = (n_k / n_k.sum()).tolist()\n\n  # Update mu\n  mu_list_next = list(((samples.T @ gamma) / n_k).T)\n\n  # Update Sigma\n  sigma_list_next = []\n  for k in range(len(pi_list)):\n    sigma_k = np.zeros_like(sigma_list[k], dtype=float)\n    for i in range(len(samples)):\n      sigma_k += gamma[i, k] * np.matmul(\n          (samples[i] - mu_list[k]).reshape(-1, 1),\n          (samples[i] - mu_list[k]).reshape(1, -1)\n      )\n\n    sigma_list_next.append(sigma_k/n_k[k])\n\n  # Why deepcopy here? (Possibly due to reference issues)\n  # Also, why is 'next' converted to regular lists for all?\n  mu_list = copy.deepcopy(mu_list_next)\n  sigma_list = copy.deepcopy(sigma_list_next)\n  pi_list = copy.deepcopy(pi_list_next)\n\n  # Convergence condition\n  likely_before = likely\n  likely = log_likelihood(mu_list, sigma_list, pi_list, samples) / NUM_DATA\n  print('Iteration: {0}, log_likelihood: {1}'.format(n_iter, likely))\n  plot_mixture_gaussian(mu_list, sigma_list, pi_list, samples, figsize=(4,3))\n\n  delta = likely - likely_before\n  if delta < th:\n    break\n\n# Display results\nprint(\"mu_list:\", mu_list)\nprint(\"sigma_list:\", sigma_list)\nprint(\"pi_list:\", pi_list)\n```\n"
  },
  {
    "title": "Welcome!",
    "summary": "Let me introduce myself and this site!",
    "tags": [
      "profile",
      "about",
      "self"
    ],
    "slug": "index",
    "content": "\n# Introduction\n\nHay! I'm Marte, a univ student.\n\nI study computer science, especially I'm into web development and machine learning (AI) like deep learning, generative model and so on.\n\nSo, I gonna think out loud what I am studying about!\n\n## Links\n\n- [Github](https://github.com/keu-5)\n- [Qiita](https://qiita.com/keu5)\n"
  }
]